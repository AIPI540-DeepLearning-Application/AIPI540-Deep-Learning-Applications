{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wplhw_Z72YE"
      },
      "source": [
        "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZA3j7RY72YK"
      },
      "source": [
        "# Deep Learning in TensorFlow\n",
        "This notebook provides an introduction to building neural networks in TensorFlow for modeling tasks using structured data.\n",
        "\n",
        "### Contents\n",
        "1) [Regression Models](#regression)  \n",
        "2) [Binary Classification Models](#binary-classification)  \n",
        "3) [Multiclass Classification](#multiclass-classification)  \n",
        "4) [Saving Models](#saving-models)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWDJAQE472YL"
      },
      "outputs": [],
      "source": [
        "# Run this cell only if working in Colab\n",
        "# Connects to any needed files from GitHub and Google Drive\n",
        "import os\n",
        "\n",
        "# Remove Colab default sample_data\n",
        "!rm -r ./sample_data\n",
        "\n",
        "# Clone GitHub files to colab workspace\n",
        "repo_name = \"AIPI540-Deep-Learning-Applications\" # Enter repo name\n",
        "git_path = 'https://github.com/AIPI540/AIPI540-Deep-Learning-Applications.git'\n",
        "!git clone \"{git_path}\"\n",
        "\n",
        "# Install dependencies from requirements.txt file\n",
        "#!pip install -r \"{os.path.join(repo_name,'requirements.txt')}\"\n",
        "\n",
        "# Change working directory to location of notebook\n",
        "notebook_dir = '1_intro_neuralnets'\n",
        "path_to_notebook = os.path.join(repo_name,notebook_dir)\n",
        "%cd \"{path_to_notebook}\"\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIvEaDt-72YP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4vFVNxax_gP"
      },
      "source": [
        "# Regression\n",
        "For regression we will use MSE as our loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCFjOphjx_gP"
      },
      "outputs": [],
      "source": [
        "# Read data in and clean up\n",
        "crimes = pd.read_csv('data/communities.csv',na_values=['?'])\n",
        "crimes.fillna(crimes.mean(),inplace=True)\n",
        "crimes.drop(columns=['state','country','community','communityname','fold'],inplace=True)\n",
        "\n",
        "X = crimes.iloc[:,:-1]\n",
        "y = crimes.iloc[:,-1]\n",
        "\n",
        "# Split our data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=0, test_size=0.2)\n",
        "\n",
        "# Define input shape\n",
        "input_shape=(X_train.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZw8XCy3x_gP"
      },
      "source": [
        "### Step 1: Set up dataloaders for our data\n",
        "The first step is to set up the dataloaders to feed our data into the model.  We will create a 'trainloder' and a 'testloader' for the training and test data which allow us to iteratively feed the data into our model in batches (called \"mini-batches\") of a size that we can specify."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_dataloaders(X_train, y_train, X_test, y_test, batch_size):\n",
        "    # Convert training and test data to TensorFlow Dataset\n",
        "    trainset = tf.data.Dataset.from_tensor_slices((np.array(X_train).astype('float32'),\n",
        "                                                  np.array(y_train).astype('float32').reshape(-1, 1)))\n",
        "    testset = tf.data.Dataset.from_tensor_slices((np.array(X_test).astype('float32'),\n",
        "                                                 np.array(y_test).astype('float32').reshape(-1, 1)))\n",
        "\n",
        "    # Shuffle and batch the training data\n",
        "    trainloader = trainset.shuffle(len(X_train)).batch(batch_size)\n",
        "    # Batch the test data\n",
        "    testloader = testset.batch(batch_size)\n",
        "\n",
        "    return trainloader, testloader\n",
        "\n",
        "# Use the function with your data\n",
        "batchsize = 32\n",
        "trainloader, testloader = prep_dataloaders(X_train, y_train, X_test, y_test, batchsize)\n"
      ],
      "metadata": {
        "id": "Sb31W7gI17d8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Define the regression network using the Tensorflow Sequential API.\n",
        "\n",
        "Compile the model with our loss = 'mean_squared_error' and set Stochastic Gradient Descent as our optimizer with a learning rate of 0.01."
      ],
      "metadata": {
        "id": "aKx66Cto_bGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the regression network using the Sequential API\n",
        "def build_regression_net(input_shape, n_hidden1, n_hidden2):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(n_hidden1, activation='relu', input_shape=(input_shape,)),\n",
        "        tf.keras.layers.Dense(n_hidden2, activation='relu'),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Instantiate the neural network\n",
        "model = build_regression_net(input_shape=input_shape, n_hidden1=50, n_hidden2=5)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
        "            loss='mean_squared_error')\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "MYB9-tdt2IFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3 - Train the Model\n",
        "\n",
        "You can use model.fit to train the model. Set verbose = 0, 1, or 2 depending on how much information you want to be able to see while training."
      ],
      "metadata": {
        "id": "xT5qiCLZA6gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of iterations (epochs) to train\n",
        "n_iter = 200\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(trainloader, epochs=n_iter, verbose=0)\n",
        "\n",
        "# Plotting the cost (loss) over epochs\n",
        "plt.plot(history.history['loss'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3zKnjQEa48hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwX2V2sx72YZ"
      },
      "source": [
        "### Step 4 - Test the model on the test set\n",
        "\n",
        "Use model.evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6sZBGXLx_gQ"
      },
      "outputs": [],
      "source": [
        "test_loss = model.evaluate(testloader)\n",
        "\n",
        "print('Test loss:', test_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkRvVd3Z72YP"
      },
      "source": [
        "# Binary classification\n",
        "For binary classification, we can use a sigmoid activation function on the output layer to get our predictions in the range (0,1) and then use binary cross entropy as our loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MO46TQAQ72YQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "data=load_breast_cancer(as_frame=True)\n",
        "X,y=data.data,data.target\n",
        "# Since the default in the file is 0=malignant 1=benign we want to reverse these\n",
        "y=(y==0).astype(int)\n",
        "X,y= np.array(X),np.array(y)\n",
        "\n",
        "# Let's set aside a test set and use the remainder for training and cross-validation\n",
        "X_train,X_test,y_train,y_test = train_test_split(X, y, random_state=0,test_size=0.2)\n",
        "\n",
        "# Let's scale our data to help the algorithm converge faster\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Set input shape of model\n",
        "input_shape=(X_train_scaled.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds for reproducibility\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "# Convert training and test data to TensorFlow Datasets\n",
        "trainloader = tf.data.Dataset.from_tensor_slices((X_train_scaled, y_train)).batch(32).shuffle(len(X_train_scaled))\n",
        "testloader = tf.data.Dataset.from_tensor_slices((X_test_scaled, y_test)).batch(32)\n"
      ],
      "metadata": {
        "id": "k6rYsNHe6_PS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the feedforward network using the Sequential API\n",
        "def build_feedforward_net(input_shape, n_hidden1, n_hidden2):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(n_hidden1, activation='relu', input_shape=(input_shape,)),\n",
        "        tf.keras.layers.Dense(n_hidden2, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Instantiate the neural network\n",
        "model = build_feedforward_net(input_shape=input_shape, n_hidden1=50, n_hidden2=20)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
        "              loss='binary_crossentropy',  # Use binary crossentropy for binary classification\n",
        "              metrics=['accuracy'])  # Track accuracy\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "qd9G63p67C9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "num_iter = 200\n",
        "history = model.fit(trainloader, epochs=num_iter,verbose=0)\n",
        "\n",
        "# Plotting the cost (loss) over epochs\n",
        "plt.plot(history.history['loss'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Cost/Loss')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HjcKkc7n7Ezs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(testloader)\n",
        "print('Test set accuracy is {:.3f}'.format(test_acc))\n",
        "\n",
        "# Predict and process the outputs\n",
        "test_predictions = model.predict(testloader)\n",
        "test_predictions = np.round(test_predictions).flatten()  # Convert probabilities to binary predictions\n",
        "test_accuracy = np.sum(test_predictions == y_test) / len(y_test)\n",
        "print('Test set accuracy calculated manually is {:.3f}'.format(test_accuracy))\n"
      ],
      "metadata": {
        "id": "QAcaygap7GXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSzP_EyO72Yb"
      },
      "source": [
        "# Multiclass classification\n",
        "For a multi-class problem we use a softmax as the activation function to convert the outputs to probabilities, rather than sigmoid as we did in binary classification.  We will use cross-entropy as the loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzQBE31q72Yb"
      },
      "outputs": [],
      "source": [
        "# Load the iris data\n",
        "iris = pd.read_csv('data/iris.csv')\n",
        "iris.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxWNUhn272Yc"
      },
      "outputs": [],
      "source": [
        "# Separate into X and y\n",
        "# Convert string species values in y to numerical codes for modeling\n",
        "X = iris.drop('species',axis=1)\n",
        "y = iris['species'].astype('category').cat.codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "100PQyXx72Yc"
      },
      "outputs": [],
      "source": [
        "# Split data into training and test sets\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)\n",
        "print(\"Shape of X_train, y_train:\",X_train.shape,y_train.shape)\n",
        "print(\"Shape of X_test, y_test:\",X_test.shape,y_test.shape)\n",
        "\n",
        "# Let's scale our data to help the algorithm converge faster\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert y_train and y_test to arrays so all inputs are in NumPy\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Define input shape\n",
        "input_shape=(X_train_scaled.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BorXvW9k72Yd"
      },
      "outputs": [],
      "source": [
        "## Set random seeds for reproducibility\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "# Convert training and test data to TensorFlow Datasets\n",
        "batch_size=32\n",
        "trainloader = tf.data.Dataset.from_tensor_slices((X_train_scaled, y_train)).batch(batch_size).shuffle(len(X_train_scaled))\n",
        "testloader = tf.data.Dataset.from_tensor_slices((X_test_scaled, y_test)).batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_multiclass_net(input_shape, n_hidden1, n_hidden2, n_hidden3, n_output):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(n_hidden1, activation='relu', input_shape=(input_shape,)),\n",
        "        tf.keras.layers.Dense(n_hidden2, activation='relu'),\n",
        "        tf.keras.layers.Dense(n_hidden3, activation='relu'),\n",
        "        tf.keras.layers.Dense(n_output, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "\n",
        "# Instantiate the neural network\n",
        "model = build_multiclass_net(input_shape=input_shape, n_hidden1=100, n_hidden2=50, n_hidden3=10, n_output=3)\n",
        "\n",
        "# Compile the model (specify the optimizer, loss, and metrics)\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "TVWgXMpV-1YP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "num_iter = 200\n",
        "history = model.fit(trainloader, epochs=num_iter,verbose=0)\n",
        "\n",
        "# Plotting the cost (loss) over epochs\n",
        "plt.plot(history.history['loss'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Cost/Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ael2nRuI_Mki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(testloader)\n",
        "print('Test set accuracy is {:.3f}'.format(test_acc))"
      ],
      "metadata": {
        "id": "buXsRAPf_lJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UL1WMSJ72Yf"
      },
      "source": [
        "## Saving models\n",
        "To save PyTorch models for later use, we have two options:  \n",
        "1) We can save the `state_dict` which contains all the learned parameters of the model (the weights and biases) but not the architecture itself.  To use it, we instantiate a new model of the desired architecture and then load the saved `state_dict` to assign values to all the parameters in the model  \n",
        "2) We can alternatively save the entire model including the architecture, and then load it up and use it for prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save/Load Entire Model (Architecture, Weights, Training Configuration, Optimizer State)\n",
        "\n",
        "This saves the architecture, weights, training configuration (loss, optimizer), and the state of the optimizer so that you can resume training where you left off."
      ],
      "metadata": {
        "id": "2Ij8FWpFOO23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving\n",
        "model.save('path_to_my_model.h5')\n",
        "\n",
        "# Loading\n",
        "from tensorflow import keras\n",
        "model = keras.models.load_model('path_to_my_model.h5')"
      ],
      "metadata": {
        "id": "tKXdVoHUOUjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Save/Load Only the Model's Weights\n",
        "\n",
        "This is useful when you need to use the same model architecture with different data or training configurations."
      ],
      "metadata": {
        "id": "zhplh3jSObgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving\n",
        "model.save_weights('path_to_my_weights.h5')\n",
        "\n",
        "# Loading\n",
        "model.load_weights('path_to_my_weights.h5')"
      ],
      "metadata": {
        "id": "hdc2kl_bOju1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Save/Load Only the Model's Architecture\n",
        "\n",
        "This method saves only the architecture of the model, not its weights or training configuration."
      ],
      "metadata": {
        "id": "Y4EdJwjfOoF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving\n",
        "json_string = model.to_json()\n",
        "with open('path_to_my_model.json', 'w') as file:\n",
        "    file.write(json_string)\n",
        "\n",
        "\n",
        "# Loading\n",
        "with open('path_to_my_model.json', 'r') as file:\n",
        "    json_string = file.read()\n",
        "model = keras.models.model_from_json(json_string)"
      ],
      "metadata": {
        "id": "nDWyefsmOtT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Save/Load Using the tf.saved_model Module\n",
        "\n",
        "This will create a SavedModel folder with a TensorFlow checkpoint and a .pb file containing the graph."
      ],
      "metadata": {
        "id": "c455HGMNOzv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving\n",
        "tf.saved_model.save(model, 'path_to_saved_model')\n",
        "\n",
        "# Loading\n",
        "loaded_model = tf.saved_model.load('path_to_saved_model')"
      ],
      "metadata": {
        "id": "ax_D9B6CO54H"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "31cc86d7aac4849c7546154c9b56d60163d5e8a1d83593a5eed18774fbf4fd37"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('aipi540')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}