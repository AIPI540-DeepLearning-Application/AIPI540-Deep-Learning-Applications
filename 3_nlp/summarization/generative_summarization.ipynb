{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstractive Text Summarization\n",
    "Abstractive text summarization methods attempt to create a summary of a document by generating shorter text which captures the main points of the source document but is much shorter in length.  Unlike extractive summarization methods, the text in summaries produced using abstractive methods may include new phrases and sentences which did not appear in the source text.\n",
    "\n",
    "The current state-of-the-art approach for abstractive text summarization uses transformer models which have been pre-trained or fine-tuned on large datasets with documents suitable for the summarization task.  In this notebook we will use the open source [Hugging Face library](https://huggingface.co) to load and use a transformer model.\n",
    "\n",
    "**Notes:**  \n",
    "- This does not need to be run on GPU, although it will take a few minutes to run on CPU\n",
    "- This notebook uses a [DistilBart model](https://arxiv.org/pdf/2010.13002.pdf), but you can also use other Bart models or Google's T5 instead  \n",
    "\n",
    "**References:**  \n",
    "- Review the Hugging Face [summarization documentation](https://huggingface.co/docs/transformers/task_summary#summarization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import requests\n",
    "from transformers import pipeline\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get document to summarize\n",
    "We will use BeautifulSoup to get the content of an article on the web and strip the text content from the hmtl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get article\n",
    "url = 'https://en.wikipedia.org/wiki/Linear_regression'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Extract body text from article\n",
    "bodytext = soup.find_all('p')\n",
    "bodytext = [i.text for i in bodytext]\n",
    "article_text = ' '.join(bodytext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model & associated tokenizer\n",
    "We will use the open source Hugging Face library to load a pre-trained transformer model from their Model Zoo.  Hugging Face recommends using a Bart or Google's T5 model for summarization tasks.  Below we will use a [DistilBart model](https://arxiv.org/pdf/2010.13002.pdf), but you can try others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate summary\n",
    "Now that our model is loaded we can use it to generate summary text.  We first tokenize the article text and then feed the tokenized text into the model to generate the summary.  We are able to specify a desired minimum and maximum length for the output summary.  Note that the DistilBart model can accept a maximum input sequence length of 1024, and so we must either truncate our source document to 1024 words or create batches of 1024 words and summarize each batch, and then combine for the full document summary.\n",
    "\n",
    "Let's first try it by simply truncating our input text to 1024 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_summary(input_text,min_length,max_length):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=max_length, min_length=min_length, length_penalty=1.0, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the source document: 2871\n",
      "Length of the summary: 53\n",
      "Summary: \n",
      "In statistics, linear regression is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables. For more than one, the process is called multiple linear regression. In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data.\n"
     ]
    }
   ],
   "source": [
    "# Set desired target min and max length for summary (note: these do not act as strict bounds)\n",
    "min_length = 50\n",
    "max_length = 200\n",
    "# Generate summary\n",
    "summary = truncate_summary(article_text,min_length,max_length)\n",
    "# Clean up output formatting\n",
    "summary = summary.split('</s>')[-2].split('<s>')[-1].strip()\n",
    "\n",
    "print('Length of the source document: {}'.format(len(article_text.split(' '))))\n",
    "print('Length of the summary: {}'.format(len(summary.split(' '))))\n",
    "print('Summary: ')\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try another approach of \"chunking\" our document into chunks of 1024 words and summarizing each chunk, and then combining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunked_summary(input_text,min_chunk_len,max_chunk_len):\n",
    "    # Separate the input text into chunks\n",
    "    input_text = input_text.split(' ')\n",
    "    chunked_inputs = [input_text[i:i+1024] for i in range(0,len(input_text),1024)]\n",
    "    summary = ''\n",
    "    # Get input for each chunk\n",
    "    for chunk in chunked_inputs:\n",
    "        chunk = ' '.join(chunk)\n",
    "        chunk_summary = truncate_summary(chunk,min_chunk_len,max_chunk_len)\n",
    "        chunk_summary = chunk_summary.split('</s>')[-2].split('<s>')[-1].strip()\n",
    "        summary += (' '+chunk_summary)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the source document: 19303\n",
      "Length of the summary: 991\n",
      "Summary: \n",
      " In statistics, linear regression is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables. For more than one, the process is called multiple linear regression. In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. The meaning of the expression \"held fixed\" may depend on how the values of the predictor variables arise. Multiple linear regression is a generalization of simple linear regression to the case of more than one independent variable. The extension to multiple and/or vector-valued predictor variables is known as multiple linear regression. Linear regression is widely used in biological, behavioral and social sciences to describe possible relationships between variables. Linear least squares methods include mainly: linear least squares. Trend lines are sometimes used in business analytics to show changes in data over time.\n"
     ]
    }
   ],
   "source": [
    "# Set desired min and max length for summary\n",
    "min_length = 25\n",
    "max_length = 100\n",
    "# Generate summary\n",
    "summary = chunked_summary(article_text,min_length,max_length)\n",
    "\n",
    "print('Length of the source document: {}'.format(len(article_text)))\n",
    "print('Length of the summary: {}'.format(len(summary)))\n",
    "print('Summary: ')\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('aipi540')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "31cc86d7aac4849c7546154c9b56d60163d5e8a1d83593a5eed18774fbf4fd37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
