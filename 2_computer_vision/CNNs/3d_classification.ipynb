{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2OkAhQG2V8n"
      },
      "source": [
        "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd6d5Y8L2V85"
      },
      "source": [
        "# 3D Classification of Medical Images\n",
        "In this example notebook we will demonstrate how to develop a classification model which uses 3-dimensional medical image files as inputs.  We will use the open-source [MONAI framework](https://monai.io/index.html, a community-supported PyTorch-based framework for deep learning in healthcare imaging, which contains some nice functionality for working with 3D medical images.\n",
        "\n",
        "Our objective in this exercise will be to try to classify MRI images into male/female.  The dateset we will be using is a very small subset of images from the [IXI Dataset](https://brain-development.org/ixi-dataset/) which contains ~600 MRI images from healthy subjects collected from three different hospitals in London.  The images are in the [NIFTI](https://radiopaedia.org/articles/nifti-file-format?lang=us) format.\n",
        "\n",
        "**Notes:**\n",
        "- This notebook should be run on GPU, but can also be run on CPU in ~10 minutes if using a small number of training epochs for demonstration purposes\n",
        "\n",
        "**References:**\n",
        "- This notebook is based on one of the [MONAI tutorials](https://github.com/Project-MONAI/tutorials).  Please see the other tutorials for additional examples of how to use the framework for various medical imaging tasks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZRokJihAd3v"
      },
      "outputs": [],
      "source": [
        "# Run this cell only if working in Colab\n",
        "# Connects to any needed files from GitHub and Google Drive\n",
        "import os\n",
        "\n",
        "# Remove Colab default sample_data\n",
        "!rm -r ./sample_data\n",
        "\n",
        "# Clone GitHub files to colab workspace\n",
        "repo_name = \"AIPI540-Deep-Learning-Applications\" # Enter repo name\n",
        "git_path = 'https://github.com/AIPI540/AIPI540-Deep-Learning-Applications.git'\n",
        "!git clone \"{git_path}\"\n",
        "\n",
        "# Install dependencies from requirements.txt file\n",
        "!pip install -r \"{os.path.join(repo_name,'requirements.txt')}\"\n",
        "\n",
        "# Change working directory to location of notebook\n",
        "notebook_dir = '2_computer_vision/CNNs'\n",
        "path_to_notebook = os.path.join(repo_name,notebook_dir)\n",
        "%cd \"{path_to_notebook}\"\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "77g7pHdf2V8s"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch:  1.10 ; cuda:  1.10.1\n"
          ]
        }
      ],
      "source": [
        "import urllib\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "import monai\n",
        "from monai.data import DataLoader, ImageDataset\n",
        "from monai.transforms import AddChannel, Compose, RandRotate90, Resize, ScaleIntensity, EnsureType\n",
        "\n",
        "pin_memory = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data preparation\n",
        "### Download data\n",
        "We are going to use a small subset of the [IXI Dataset](https://brain-development.org/ixi-dataset/).  Note that this data is made available under the Creative Commons [CC BY-SA 3.0 license](https://creativecommons.org/licenses/by-sa/3.0/legalcode).  Start by running the cell below to download the needed image files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the data\n",
        "if not os.path.exists('./data'):\n",
        "    os.mkdir('./data')\n",
        "if not os.path.exists('./data/ixi'):\n",
        "    url = 'https://storage.googleapis.com/aipi540-datasets/ixi.zip'\n",
        "    destfile = 'data/ixi.zip'\n",
        "    urllib.request.urlretrieve(url,filename=destfile)\n",
        "    #Unzip file to path\n",
        "    zip_ref = zipfile.ZipFile(destfile, 'r')\n",
        "    zip_ref.extractall('data/')\n",
        "    zip_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add labels\n",
        "Since we are working with a very small subsample we will manually input the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "images = [\n",
        "        \"IXI314-IOP-0889-T1.nii.gz\",\n",
        "        \"IXI249-Guys-1072-T1.nii.gz\",\n",
        "        \"IXI609-HH-2600-T1.nii.gz\",\n",
        "        \"IXI173-HH-1590-T1.nii.gz\",\n",
        "        \"IXI020-Guys-0700-T1.nii.gz\",\n",
        "        \"IXI342-Guys-0909-T1.nii.gz\",\n",
        "        \"IXI134-Guys-0780-T1.nii.gz\",\n",
        "        \"IXI577-HH-2661-T1.nii.gz\",\n",
        "        \"IXI066-Guys-0731-T1.nii.gz\",\n",
        "        \"IXI130-HH-1528-T1.nii.gz\",\n",
        "        \"IXI607-Guys-1097-T1.nii.gz\",\n",
        "        \"IXI175-HH-1570-T1.nii.gz\",\n",
        "        \"IXI385-HH-2078-T1.nii.gz\",\n",
        "        \"IXI344-Guys-0905-T1.nii.gz\",\n",
        "        \"IXI409-Guys-0960-T1.nii.gz\",\n",
        "        \"IXI584-Guys-1129-T1.nii.gz\",\n",
        "        \"IXI253-HH-1694-T1.nii.gz\",\n",
        "        \"IXI092-HH-1436-T1.nii.gz\",\n",
        "        \"IXI574-IOP-1156-T1.nii.gz\",\n",
        "        \"IXI585-Guys-1130-T1.nii.gz\",\n",
        "    ]\n",
        "\n",
        "datapath = os.path.join('data','ixi')\n",
        "images = [os.sep.join([datapath, f]) for f in images]\n",
        "\n",
        "# Create binary labels for man/woman classification\n",
        "labels = np.array([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0], dtype=np.int64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create PyTorch Datasets from data and load DataLoaders\n",
        "Rather than using PyTorch's base implementations of the Dataset and DataLoader, we will use MONAI's DataSet and DataLoader which are wrappers around their PyTorch equivalents which add some additional functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image type: <class 'torch.Tensor'>\n",
            "Input batch shape: torch.Size([2, 1, 96, 96, 96])\n",
            "Label batch shape: torch.Size([2])\n"
          ]
        }
      ],
      "source": [
        "# Define transforms\n",
        "train_transforms = Compose([ScaleIntensity(), AddChannel(), Resize((96, 96, 96)), RandRotate90(), EnsureType()])\n",
        "val_transforms = Compose([ScaleIntensity(), AddChannel(), Resize((96, 96, 96)), EnsureType()])\n",
        "\n",
        "# Create training Dataset and DataLoader using first 10 images\n",
        "train_ds = ImageDataset(image_files=images[:10], labels=labels[:10], transform=train_transforms)\n",
        "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=2, pin_memory=torch.cuda.is_available())\n",
        "\n",
        "# Create validation Dataset and DataLoader using the rest of the images\n",
        "val_ds = ImageDataset(image_files=images[-10:], labels=labels[-10:], transform=val_transforms)\n",
        "val_loader = DataLoader(val_ds, batch_size=2, num_workers=2, pin_memory=torch.cuda.is_available())\n",
        "\n",
        "# Set up dict for dataloaders\n",
        "dataloaders = {'train':train_loader,'val':val_loader}\n",
        "\n",
        "# Store size of training and validation sets\n",
        "dataset_sizes = {'train':len(train_ds),'val':len(val_ds)}\n",
        "\n",
        "im, label = monai.utils.misc.first(train_loader)\n",
        "print(f'Image type: {type(im)}')\n",
        "print(f'Input batch shape: {im.shape}')\n",
        "print(f'Label batch shape: {label.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we see above, the 3D inputs in our DataLoader are of shape [N,C,H,W,D) where:  \n",
        "- N = batch size  \n",
        "- C = number of channels (1 in this case for grayscale)  \n",
        "- H = image height  \n",
        "- W = image width\n",
        "- D = image depth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define our model architecture\n",
        "We will used a pre-trained DenseNet 121 model for this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a pre-trained DenseNet121\n",
        "# We have a signle input channel, and we have 2 output classes\n",
        "# We set spatial_dims=3 to indicate we want to use the version suitable for 3D input images\n",
        "model = monai.networks.nets.DenseNet121(spatial_dims=3, in_channels=1, out_channels=2).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, dataloaders, device, num_epochs=5):\n",
        "\n",
        "    model = model.to(device) # Send model to GPU if available\n",
        "\n",
        "    iter_num = {'train':0,'val':0} # Track total number of iterations\n",
        "\n",
        "    best_metric = -1\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Get the input images and labels, and send to GPU if available\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Zero the weight gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass to get outputs and calculate loss\n",
        "                # Track gradient only for training data\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Backpropagation to get the gradients with respect to each weight\n",
        "                    # Only if in train\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        # Update the weights\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Convert loss into a scalar and add it to running_loss\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                # Track number of correct predictions\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "                # Iterate count of iterations\n",
        "                iter_num[phase] += 1\n",
        "\n",
        "            # Calculate and display average loss and accuracy for the epoch\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # Save weights if accuracy is best\n",
        "            if phase=='val':\n",
        "                if epoch_acc > best_metric:\n",
        "                    best_metric = epoch_acc\n",
        "                    if not os.path.exists('./models'):\n",
        "                        os.mkdir('./models')\n",
        "                    torch.save(model.state_dict,'models/3d_classification_model.pth')\n",
        "                    print('Saved best new model')\n",
        "\n",
        "    print(f'Training complete. Best validation set accuracy was {best_metric}')\n",
        "    \n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/4\n",
            "----------\n",
            "train Loss: 0.7231 Acc: 0.5000\n",
            "val Loss: 0.6714 Acc: 0.6000\n",
            "Saved best new model\n",
            "Epoch 1/4\n",
            "----------\n",
            "train Loss: 0.5776 Acc: 0.7000\n",
            "val Loss: 0.6721 Acc: 0.6000\n",
            "Epoch 2/4\n",
            "----------\n",
            "train Loss: 0.6822 Acc: 0.7000\n",
            "val Loss: 0.6755 Acc: 0.6000\n",
            "Epoch 3/4\n",
            "----------\n",
            "train Loss: 0.5370 Acc: 0.7000\n",
            "val Loss: 0.6817 Acc: 0.5000\n",
            "Epoch 4/4\n",
            "----------\n",
            "train Loss: 0.5039 Acc: 0.8000\n",
            "val Loss: 0.6553 Acc: 0.6000\n",
            "Training complete. Best validation set accuracy was 0.6\n"
          ]
        }
      ],
      "source": [
        "# Use cross-entropy loss function\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "# loss_function = torch.nn.BCEWithLogitsLoss()  # also works with this data\n",
        "\n",
        "# Use Adam adaptive optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
        "\n",
        "# Train the model\n",
        "epochs=5\n",
        "train_model(model, criterion, optimizer, dataloaders, device, num_epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "cnn_basics.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('aipi540')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "31cc86d7aac4849c7546154c9b56d60163d5e8a1d83593a5eed18774fbf4fd37"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
