{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2OkAhQG2V8n"
      },
      "source": [
        "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd6d5Y8L2V85"
      },
      "source": [
        "# 3D Classification of Medical Images\n",
        "In this example notebook we are demonstrate how to develop a classification model on 3-dimensional medical image files.  We will use the open-source [MONAI framework](https://monai.io/index.html) to help us.  MonAI is a community-supported PyTorch-based framework for deep learning in healthcare imaging.\n",
        "\n",
        "Our objective in this exercise will be to try to classify MR images into male/female.  THe dateset we will be using is a very small subset of images from the [IXI Dataset](https://brain-development.org/ixi-dataset/) which contains ~600 MR images from healthy subjects collected from three different hospitals in London.  The images are in [NIFTI](https://radiopaedia.org/articles/nifti-file-format?lang=us) format.\n",
        "\n",
        "**Notes:**\n",
        "- This notebook must be run on GPU\n",
        "\n",
        "**References:**\n",
        "- This notebook is based on one of the [MONAI tutorials](https://github.com/Project-MONAI/tutorials).  Please see the other tutorials for additional examples of how to use the framework for various medical imaging tasks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZRokJihAd3v"
      },
      "outputs": [],
      "source": [
        "# Run this cell only if working in Colab\n",
        "# Connects to any needed files from GitHub and Google Drive\n",
        "import os\n",
        "\n",
        "# Remove Colab default sample_data\n",
        "!rm -r ./sample_data\n",
        "\n",
        "# Clone GitHub files to colab workspace\n",
        "repo_name = \"AIPI540-Deep-Learning-Applications\" # Enter repo name\n",
        "git_path = 'https://github.com/AIPI540/AIPI540-Deep-Learning-Applications.git'\n",
        "!git clone \"{git_path}\"\n",
        "\n",
        "# Install dependencies from requirements.txt file\n",
        "#!pip install -r \"{os.path.join(repo_name,'requirements.txt')}\"\n",
        "\n",
        "# Change working directory to location of notebook\n",
        "notebook_dir = '2_computer_vision/CNNs'\n",
        "path_to_notebook = os.path.join(repo_name,notebook_dir)\n",
        "%cd \"{path_to_notebook}\"\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "77g7pHdf2V8s"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch:  1.10 ; cuda:  1.10.1\n"
          ]
        }
      ],
      "source": [
        "import urllib\n",
        "import zipfile\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "import monai\n",
        "from monai.apps import download_and_extract\n",
        "from monai.config import print_config\n",
        "from monai.data import DataLoader, ImageDataset\n",
        "from monai.transforms import AddChannel, Compose, RandRotate90, Resize, ScaleIntensity, EnsureType\n",
        "\n",
        "pin_memory = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data preparation\n",
        "### Download data\n",
        "We are going to use a small subset of the [IXI Dataset](https://brain-development.org/ixi-dataset/).  Note that this data is made available under the Creative Commons [CC BY-SA 3.0 license](https://creativecommons.org/licenses/by-sa/3.0/legalcode).  Start by running the cell below to download the needed image files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the data\n",
        "if not os.path.exists('./data'):\n",
        "    os.mkdir('./data')\n",
        "if not os.path.exists('./data/ixi'):\n",
        "    url = 'https://storage.googleapis.com/aipi540-datasets/ixi.zip'\n",
        "    destfile = 'data/ixi.zip'\n",
        "    urllib.request.urlretrieve(url,filename=destfile)\n",
        "    #Unzip file to path\n",
        "    zip_ref = zipfile.ZipFile(destfile, 'r')\n",
        "    zip_ref.extractall('data/')\n",
        "    zip_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add labels\n",
        "Since we are working with a very small subsample we will manually input the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "images = [\n",
        "        \"IXI314-IOP-0889-T1.nii.gz\",\n",
        "        \"IXI249-Guys-1072-T1.nii.gz\",\n",
        "        \"IXI609-HH-2600-T1.nii.gz\",\n",
        "        \"IXI173-HH-1590-T1.nii.gz\",\n",
        "        \"IXI020-Guys-0700-T1.nii.gz\",\n",
        "        \"IXI342-Guys-0909-T1.nii.gz\",\n",
        "        \"IXI134-Guys-0780-T1.nii.gz\",\n",
        "        \"IXI577-HH-2661-T1.nii.gz\",\n",
        "        \"IXI066-Guys-0731-T1.nii.gz\",\n",
        "        \"IXI130-HH-1528-T1.nii.gz\",\n",
        "        \"IXI607-Guys-1097-T1.nii.gz\",\n",
        "        \"IXI175-HH-1570-T1.nii.gz\",\n",
        "        \"IXI385-HH-2078-T1.nii.gz\",\n",
        "        \"IXI344-Guys-0905-T1.nii.gz\",\n",
        "        \"IXI409-Guys-0960-T1.nii.gz\",\n",
        "        \"IXI584-Guys-1129-T1.nii.gz\",\n",
        "        \"IXI253-HH-1694-T1.nii.gz\",\n",
        "        \"IXI092-HH-1436-T1.nii.gz\",\n",
        "        \"IXI574-IOP-1156-T1.nii.gz\",\n",
        "        \"IXI585-Guys-1130-T1.nii.gz\",\n",
        "    ]\n",
        "\n",
        "datapath = os.path.join('data','ixi')\n",
        "images = [os.sep.join([datapath, f]) for f in images]\n",
        "\n",
        "# Create binary labels for man/woman classification\n",
        "labels = np.array([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0], dtype=np.int64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create PyTorch Datasets from data and load DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define transforms\n",
        "train_transforms = Compose([ScaleIntensity(), AddChannel(), Resize((96, 96, 96)), RandRotate90(), EnsureType()])\n",
        "val_transforms = Compose([ScaleIntensity(), AddChannel(), Resize((96, 96, 96)), EnsureType()])\n",
        "\n",
        "# Create training Dataset and DataLoader using first 10 images\n",
        "train_ds = ImageDataset(image_files=images[:10], labels=labels[:10], transform=train_transforms)\n",
        "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=2, pin_memory=torch.cuda.is_available())\n",
        "\n",
        "# Create validation Dataset and DataLoader using the rest of the images\n",
        "val_ds = ImageDataset(image_files=images[-10:], labels=labels[-10:], transform=val_transforms)\n",
        "val_loader = DataLoader(val_ds, batch_size=2, num_workers=2, pin_memory=torch.cuda.is_available())\n",
        "\n",
        "# Set up dict for dataloaders\n",
        "dataloaders = {'train':train_loader,'val':val_loader}\n",
        "\n",
        "# Store size of training and validation sets\n",
        "dataset_sizes = {'train':len(train_ds),'val':len(val_ds)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we see above, the inputs in a PyTorch DataLoader are of shape [N,C,H,W,D) where:  \n",
        "- N = batch size  \n",
        "- C = number of channels (1 in this case for grayscale)  \n",
        "- H = image height  \n",
        "- W = image width\n",
        "- D = image depth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define our model architecture\n",
        "We will used a pre-trained ResNet18 model in this example.  However, we will need to make a couple changes to it:  \n",
        "- Since we have 2 output classes instead of 1000 (the ImageNet default), we need to replace the final fully connected layer in the network with a new layer that has 2 output units (not 1000)  \n",
        "- Since our images are black/white, we have a single input channel rather than 3.  Therefore we will replace the first layer in the network with a new convolutional layer that has `in_channels = 1`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a pre-trained DenseNet121\n",
        "model = monai.networks.nets.DenseNet121(spatial_dims=3, in_channels=1, out_channels=2).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, dataloaders, device, num_epochs=5):\n",
        "\n",
        "    model = model.to(device) # Send model to GPU if available\n",
        "\n",
        "    iter_num = {'train':0,'val':0} # Track total number of iterations\n",
        "\n",
        "    best_metric = -1\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Get the input images and labels, and send to GPU if available\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Zero the weight gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass to get outputs and calculate loss\n",
        "                # Track gradient only for training data\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Backpropagation to get the gradients with respect to each weight\n",
        "                    # Only if in train\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        # Update the weights\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Convert loss into a scalar and add it to running_loss\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                # Track number of correct predictions\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "                # Iterate count of iterations\n",
        "                iter_num[phase] += 1\n",
        "\n",
        "            # Calculate and display average loss and accuracy for the epoch\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # Save weights if accuracy is best\n",
        "            if phase=='val':\n",
        "                if epoch_acc > best_metric:\n",
        "                    torch.save(model.state_dict,'models/3d_classification_model.pth')\n",
        "                    print('Saved best new model')\n",
        "\n",
        "    print(f'Training complete. Best validation set accuracy was {best_metric}')\n",
        "    \n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use cross-entropy loss function\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "# loss_function = torch.nn.BCEWithLogitsLoss()  # also works with this data\n",
        "\n",
        "# Use Adam adaptive optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
        "\n",
        "# Train the model\n",
        "epochs=5\n",
        "train_model(model, criterion, optimizer, dataloaders, device, num_epochs=epochs)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "cnn_basics.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('aipi540')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "31cc86d7aac4849c7546154c9b56d60163d5e8a1d83593a5eed18774fbf4fd37"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
