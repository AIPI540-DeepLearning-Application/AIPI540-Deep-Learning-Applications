{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2OkAhQG2V8n"
      },
      "source": [
        "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3bKXytY2V8r"
      },
      "source": [
        "# ResNet Example\n",
        "In this demo notebook we are going to demonstrate the creation and use of a [ResNet model](https://arxiv.org/pdf/1512.03385.pdf) to see how the ResNet's skip connections can be implemented in code.  The implementations demonstrated here are shallow ResNet model rather than the deep ResNet architectures described in the paper (and available in the [PyTorch model zoo](https://pytorch.org/vision/stable/models.html)).\n",
        "\n",
        "We will implement ResNet two different ways in this notebook:  \n",
        "1) Network with no resizing of dimensions in the main path, and pass-through skip connections \n",
        "2) Network with resizing of dimensions in the main path, and skip connections using a convolutional layer for resizing to ensure equal size at combination\n",
        "\n",
        "**Notes:**\n",
        "- This notebook should be run on GPU\n",
        "\n",
        "**References:**\n",
        "- Read the original [ResNet paper](https://arxiv.org/pdf/1512.03385.pdf) for details on the ResNet architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZRokJihAd3v"
      },
      "outputs": [],
      "source": [
        "# Run this cell only if working in Colab\n",
        "# Connects to any needed files from GitHub and Google Drive\n",
        "import os\n",
        "\n",
        "# Remove Colab default sample_data\n",
        "!rm -r ./sample_data\n",
        "\n",
        "# Clone GitHub files to colab workspace\n",
        "repo_name = \"AIPI540-Deep-Learning-Applications\" # Enter repo name\n",
        "git_path = 'https://github.com/AIPI540/AIPI540-Deep-Learning-Applications.git'\n",
        "!git clone \"{git_path}\"\n",
        "\n",
        "# Install dependencies from requirements.txt file\n",
        "#!pip install -r \"{os.path.join(repo_name,'requirements.txt')}\"\n",
        "\n",
        "# Change working directory to location of notebook\n",
        "notebook_dir = 'computer_vision/CNNs'\n",
        "path_to_notebook = os.path.join(repo_name,notebook_dir)\n",
        "%cd \"{path_to_notebook}\"\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77g7pHdf2V8s"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torchsummary import summary\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get data and set up dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the data\n",
        "if not os.path.exists('./data'):\n",
        "    os.mkdir('./data')\n",
        "if not os.path.exists('data/hymenoptera_data'):\n",
        "    url = 'https://download.pytorch.org/tutorial/hymenoptera_data.zip'\n",
        "    urllib.request.urlretrieve(url,filename='data/hymenoptera_data.zip')\n",
        "    zip_ref = zipfile.ZipFile('data/hymenoptera_data.zip', 'r')\n",
        "    zip_ref.extractall('data/')\n",
        "    zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = 'data/hymenoptera_data'\n",
        "\n",
        "# Set up transformations for training and validation (test) data\n",
        "# For training data we will do randomized cropping to get to 224 * 224, randomized horizontal flipping, and normalization\n",
        "# For test set we will do only center cropping to get to 224 * 224 and normalization\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Create Datasets for training and validation sets\n",
        "train_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'),\n",
        "                                          data_transforms['train'])\n",
        "val_dataset = datasets.ImageFolder(os.path.join(data_dir, 'val'),\n",
        "                                          data_transforms['val'])\n",
        "\n",
        "# Create DataLoaders for training and validation sets\n",
        "batch_size = 4\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
        "                                             shuffle=True, num_workers=4)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,\n",
        "                                             shuffle=False, num_workers=4)\n",
        "\n",
        "# Set up dict for dataloaders\n",
        "dataloaders = {'train':train_loader,'val':val_loader}\n",
        "# Store size of training and validation sets\n",
        "dataset_sizes = {'train':len(train_dataset),'val':len(val_dataset)}\n",
        "# Get class names associated with labels\n",
        "class_names = train_dataset.classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "images, labels = iter(train_loader).next()\n",
        "images = images.numpy()\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "for idx in np.arange(batch_size):\n",
        "    ax = fig.add_subplot(2, batch_size//2, idx+1, xticks=[], yticks=[])\n",
        "    image = images[idx]\n",
        "    image = image.transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    image = std * image + mean\n",
        "    image = np.clip(image, 0, 1)\n",
        "    ax.imshow(image)\n",
        "    ax.set_title(\"{}\".format(class_names[labels[idx]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3EGj5MS2V8w"
      },
      "source": [
        "## Version 1: Simple ResNet with passthrough skip connections\n",
        "Let's first create a ResNet with no resizing of the dimensions in the main path, which means we can use simple skip connections that just pass the block input directly through an identity block without resizing it.\n",
        "\n",
        "![](img/resnet1.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3yfba9A2V8x"
      },
      "outputs": [],
      "source": [
        "class ResNetSimple(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        ### DOWNSCALING LAYER ###\n",
        "\n",
        "        # Conv 1 layer: (3,224,224) -> (8,224,224)\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
        "        # Conv1 layer output size = (W-F+2P)/S+1 = (224-3+2)/1+1 = 224\n",
        "        # Batch norm\n",
        "        self.bn1 = nn.BatchNorm2d(8)\n",
        "        # Maxpool layer: (8,224,224) -> (8,112,112)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        \n",
        "        ### RESNET BLOCK 1 ###\n",
        "\n",
        "        # Conv layer: (8,112,112) -> (8,112,112)\n",
        "        self.block1conv1 = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
        "        # Batch norm\n",
        "        self.block1bn1 = nn.BatchNorm2d(8)\n",
        "        # Conv layer: (8,112,112) -> (8,112,112)\n",
        "        self.block1conv2 = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
        "        # Batch norm\n",
        "        self.block1bn2 = nn.BatchNorm2d(8)\n",
        "\n",
        "        ### RESNET BLOCK 2 ###\n",
        "\n",
        "        # Conv layer: (8,112,112) -> (8,112,112)\n",
        "        self.block2conv1 = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
        "        # Batch norm\n",
        "        self.block2bn1 = nn.BatchNorm2d(8)\n",
        "        # Conv layer: (8,112,112) -> (8,112,112)\n",
        "        self.block2conv2 = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
        "        # Batch norm\n",
        "        self.block2bn2 = nn.BatchNorm2d(8)\n",
        "\n",
        "        ### FINAL LAYERS ###\n",
        "\n",
        "        # Average pooling layer: (8,112,112) -> (8,16,16)\n",
        "        self.pool2 = nn.AvgPool2d(kernel_size=7, stride=7)\n",
        "        \n",
        "        # Input size: 8 * 16 * 16 = 2048 from pooling layer\n",
        "        # 2 output channels (for the 2 classes)\n",
        "        self.fc1 = nn.Linear(8*16*16, 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "\n",
        "        ### DOWNSCALING LAYER ###\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        ### RESNET BLOCK 1 ###\n",
        "        skipconnect = x\n",
        "\n",
        "        x_out = self.block1conv1(x) # conv1\n",
        "        x_out = self.block1bn1(x_out) # batch norm 1\n",
        "        x_out = F.relu(x_out) # relu\n",
        "\n",
        "        x_out = self.block1conv2(x_out) # conv2\n",
        "        x_out = self.block1bn2(x_out) # batch norm 2\n",
        "\n",
        "        # Add layer and skipconnect, then activation\n",
        "        x_out += skipconnect\n",
        "        x_out = F.relu(x_out)\n",
        "\n",
        "        ### RESNET BLOCK 2 ###\n",
        "        skipconnect = x_out\n",
        "\n",
        "        x_out = self.block2conv1(x_out) # conv1\n",
        "        x_out = self.block2bn1(x_out) # batch norm 1\n",
        "        x_out = F.relu(x_out) # relu\n",
        "\n",
        "        x_out = self.block2conv2(x_out) # conv2\n",
        "        x_out = self.block2bn2(x_out) # batch norm 2\n",
        "\n",
        "        # Add layer and skipconnect, then activation\n",
        "        x_out += skipconnect\n",
        "        x_out = F.relu(x_out)\n",
        "\n",
        "        ### FINAL LAYERS ###\n",
        "        x_out = self.pool2(x_out)\n",
        "        # Flatten into a vector to feed into linear layer\n",
        "        x_out = x_out.view(x_out.size(0), -1)\n",
        "        # Linear layer\n",
        "        x_out = self.fc1(x_out)\n",
        "        \n",
        "        return x_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7q1hclmU2V8y"
      },
      "outputs": [],
      "source": [
        "# Instantiate the model\n",
        "net = ResNetSimple()\n",
        "\n",
        "# Display a summary of the layers of the model and output shape after each layer\n",
        "summary(net,(images.shape[1:]),batch_size=batch_size,device=\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ARCSO3Z2V8z"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, dataloaders, device, num_epochs=50):\n",
        "\n",
        "    model = model.to(device) # Send model to GPU if available\n",
        "\n",
        "    iter_num = {'train':0,'val':0} # Track total number of iterations\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Get the input images and labels, and send to GPU if available\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Zero the weight gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass to get outputs and calculate loss\n",
        "                # Track gradient only for training data\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Backpropagation to get the gradients with respect to each weight\n",
        "                    # Only if in train\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        # Update the weights\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Convert loss into a scalar and add it to running_loss\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                # Track number of correct predictions\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "                # Iterate count of iterations\n",
        "                iter_num[phase] += 1\n",
        "\n",
        "            # Calculate and display average loss and accuracy for the epoch\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "    \n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate the model\n",
        "net = ResNetSimple()\n",
        "\n",
        "# Cross entropy loss combines softmax and nn.NLLLoss() in one single class.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = optim.Adam(net.parameters(),  lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train_model(net, criterion, optimizer, dataloaders, device, num_epochs=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "9bGA5Jgx2V83",
        "outputId": "5eef2a8b-dcf8-4c48-ce5b-6eb197072bde"
      },
      "outputs": [],
      "source": [
        "# Display a batch of predictions\n",
        "\n",
        "def visualize_results(model,dataloader,device):\n",
        "    model = model.to(device) # Send model to GPU if available\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        # Get a batch of validation images\n",
        "        images, labels = iter(val_loader).next()\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        # Get predictions\n",
        "        _,preds = torch.max(model(images), 1)\n",
        "        preds = np.squeeze(preds.cpu().numpy())\n",
        "        images = images.cpu().numpy()\n",
        "\n",
        "    # Plot the images in the batch, along with predicted and true labels\n",
        "    fig = plt.figure(figsize=(15, 10))\n",
        "    for idx in np.arange(len(preds)):\n",
        "        ax = fig.add_subplot(2, len(preds)//2, idx+1, xticks=[], yticks=[])\n",
        "        image = images[idx]\n",
        "        image = image.transpose((1, 2, 0))\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        image = std * image + mean\n",
        "        image = np.clip(image, 0, 1)\n",
        "        ax.imshow(image)\n",
        "        ax.set_title(\"{} ({})\".format(class_names[preds[idx]], class_names[labels[idx]]),\n",
        "                    color=(\"green\" if preds[idx]==labels[idx] else \"red\"))\n",
        "    return\n",
        "\n",
        "visualize_results(net,val_loader,device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3EGj5MS2V8w"
      },
      "source": [
        "## Version 2: ResNet with resizing\n",
        "Let's now create a new ResNet architecture which resizes the input dimensions in the main path (reducing the image dimensions while increasing number of feature maps).  This means we also need to apply a convolution layer in the skip connection branch to resize the data flowing through it, so that its dimensions match the dimensions of the data flowing through the main path in order for them to be combined together.\n",
        "\n",
        "![](img/resnet2.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3yfba9A2V8x"
      },
      "outputs": [],
      "source": [
        "class ResNetResize(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        ### DOWNSCALING LAYER ###\n",
        "\n",
        "        # Conv layer: (3,224,224) -> (8,224,224)\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
        "        # Conv layer output size = (W-F+2P)/S+1 = (224-3+2)/1+1 = 224\n",
        "        self.bn1 = nn.BatchNorm2d(8)\n",
        "        \n",
        "        ### RESNET BLOCK 1 ###\n",
        "\n",
        "        # Main path\n",
        "        # Conv layer: (8,224,224) -> (16,112,112)\n",
        "        self.block1conv1 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=2, padding=1)\n",
        "        self.block1bn1 = nn.BatchNorm2d(16)\n",
        "        # Conv layer: (16,112,112) -> (16,112,112)\n",
        "        self.block1conv2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.block1bn2 = nn.BatchNorm2d(16)\n",
        "\n",
        "        # Skip connection: (8,224,224) -> (16,112,112)\n",
        "        self.block1skipconv = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=2, padding=1)\n",
        "        self.block1skipbn = nn.BatchNorm2d(16)\n",
        "\n",
        "        ### RESNET BLOCK 2 ###\n",
        "\n",
        "        # Main path\n",
        "        # Conv layer: (16,112,112) -> (32,56,56)\n",
        "        self.block2conv1 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
        "        self.block2bn1 = nn.BatchNorm2d(32)\n",
        "        # Conv layer: (32,56,56) -> (32,56,56)\n",
        "        self.block2conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.block2bn2 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Skip connection: (16,112,112) -> (32,56,56)\n",
        "        self.block2skipconv = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
        "        self.block2skipbn = nn.BatchNorm2d(32)\n",
        "\n",
        "        ### FINAL LAYERS ###\n",
        "\n",
        "        # Average pooling layer: (32,56,56) -> (32,8,8)\n",
        "        self.pool2 = nn.AvgPool2d(kernel_size=7, stride=7)\n",
        "        \n",
        "        # Input size: 32 * 8 * 8 =  from pooling layer\n",
        "        # 2 output channels (for the 2 classes)\n",
        "        self.fc1 = nn.Linear(32*8*8, 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "\n",
        "        ### DOWNSCALING LAYER ###\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        ### RESNET BLOCK 1 ###\n",
        "        skipconnect = x\n",
        "        skipconnect = self.block1skipconv(skipconnect)\n",
        "        skipconnect = self.block1skipbn(skipconnect)\n",
        "\n",
        "        x_out = self.block1conv1(x) # conv1\n",
        "        x_out = self.block1bn1(x_out) # batch norm 1\n",
        "        x_out = F.relu(x_out) # relu\n",
        "\n",
        "        x_out = self.block1conv2(x_out) # conv2\n",
        "        x_out = self.block1bn2(x_out) # batch norm 2\n",
        "\n",
        "        # Add layer and skipconnect, then activation\n",
        "        x_out += skipconnect\n",
        "        x_out = F.relu(x_out)\n",
        "\n",
        "        ### RESNET BLOCK 2 ###\n",
        "        skipconnect = x_out\n",
        "        skipconnect = self.block2skipconv(skipconnect)\n",
        "        skipconnect = self.block2skipbn(skipconnect)\n",
        "\n",
        "        x_out = self.block2conv1(x_out) # conv1\n",
        "        x_out = self.block2bn1(x_out) # batch norm 1\n",
        "        x_out = F.relu(x_out) # relu\n",
        "\n",
        "        x_out = self.block2conv2(x_out) # conv2\n",
        "        x_out = self.block2bn2(x_out) # batch norm 2\n",
        "\n",
        "        # Add layer and skipconnect, then activation\n",
        "        x_out += skipconnect\n",
        "        x_out = F.relu(x_out)\n",
        "\n",
        "        ### FINAL LAYERS ###\n",
        "        x_out = self.pool2(x_out)\n",
        "        # Flatten into a vector to feed into linear layer\n",
        "        x_out = x_out.view(x_out.size(0), -1)\n",
        "        # Linear layer\n",
        "        x_out = self.fc1(x_out)\n",
        "        \n",
        "        return x_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate the model\n",
        "net = ResNetResize()\n",
        "\n",
        "# Cross entropy loss combines softmax and nn.NLLLoss() in one single class.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = optim.Adam(net.parameters(),  lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train_model(net, criterion, optimizer, dataloaders, device, num_epochs=100)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "cnn_basics.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
