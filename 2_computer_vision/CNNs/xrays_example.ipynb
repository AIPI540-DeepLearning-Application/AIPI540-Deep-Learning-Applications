{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2OkAhQG2V8n"
      },
      "source": [
        "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd6d5Y8L2V85"
      },
      "source": [
        "# Identifying Pneumothorax in Chest X-rays\n",
        "In this example notebook we are going to bring together several of the things we have learned about related to image classification and focus on two aspects that are critical for modeling real-world problems:  \n",
        "- Creating a custom Dataset in PyTorch to load images and labels in non-standard formats  \n",
        "- Loading and fine-tuning a pretrained model on a new task  \n",
        "\n",
        "Our objective in this exercise will be to identify [pneumothorax](https://en.wikipedia.org/wiki/Pneumothorax) from chest X-ray images. \n",
        "\n",
        "The data we will be working with is a set of chest X-rays in the DICOM (Digital Imaging and Communications in Medicine) format.  DICOM is a standard for medical images (X-rays, MRI, CT) which allows information to be exchanged between different imaging equipment and hospitals.  DICOM files include both a header and image data and have the file extension `.dcm`.  The file header can contain equipment information, patient information, study information, etc.\n",
        "\n",
        "The particular dataset we will work with is a subset of 250 DICOM files from the [NIH Chest X-ray Dataset](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7165201/), which includes 111,000 total chest X-ray images and associated labels of various diseases.  Of these a subset of 15302 was taken to include a mix of images labeled pneumothorax or normal.  The 250 we will use representes a small fraction of this subset.\n",
        "\n",
        "**Notes:**\n",
        "- This notebook is recommended to be run on GPU but can be run on CPU in 10-15 minutes for demo training  \n",
        "- Actual training should be run for many more epochs and done on GPU\n",
        "\n",
        "**References:**\n",
        "- This notebook is inspired by the example in the [fast.ai documentation](https://docs.fast.ai/tutorial.medical_imaging.html).  Check out their tutorial if you'd like to use the fast.ai framework rather than pure PyTorch  \n",
        "- Read the [original paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7165201/) by Filice et al. about this dataset\n",
        "- This example uses a ResNet18 architecture pre-trained on ImageNet.  Read the original [ResNet paper](https://arxiv.org/pdf/1512.03385.pdf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZRokJihAd3v"
      },
      "outputs": [],
      "source": [
        "# Run this cell only if working in Colab\n",
        "# Connects to any needed files from GitHub and Google Drive\n",
        "import os\n",
        "\n",
        "# Remove Colab default sample_data\n",
        "!rm -r ./sample_data\n",
        "\n",
        "# Clone GitHub files to colab workspace\n",
        "repo_name = \"AIPI540-Deep-Learning-Applications\" # Enter repo name\n",
        "git_path = 'https://github.com/AIPI540/AIPI540-Deep-Learning-Applications.git'\n",
        "!git clone \"{git_path}\"\n",
        "\n",
        "# Install dependencies from requirements.txt file\n",
        "#!pip install -r \"{os.path.join(repo_name,'requirements.txt')}\"\n",
        "\n",
        "# Change working directory to location of notebook\n",
        "notebook_dir = '2_computer_vision/CNNs'\n",
        "path_to_notebook = os.path.join(repo_name,notebook_dir)\n",
        "%cd \"{path_to_notebook}\"\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77g7pHdf2V8s"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import tarfile\n",
        "import copy\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torchsummary import summary\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import pydicom\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data preparation\n",
        "### Download data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the data\n",
        "if not os.path.exists('./data'):\n",
        "    os.mkdir('./data')\n",
        "if not os.path.exists('data/siim_small'):\n",
        "    url = 'https://s3.amazonaws.com/fast-ai-imagelocal/siim_small.tgz'\n",
        "    urllib.request.urlretrieve(url,filename='data/siim_small.tgz')\n",
        "    file = tarfile.open('data/siim_small.tgz')\n",
        "    file.extractall('data')\n",
        "    file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convert labels to numeric\n",
        "Since PyTorch cannot work with string labels, we need to set up a mapping dictionary to convert them to integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classes = ['No Pneumothorax','Pneumothorax']\n",
        "idx_to_class = {i:j for i,j in enumerate(classes)}\n",
        "class_to_idx = {v:k for k,v in idx_to_class.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create PyTorch Datasets from data and load DataLoaders\n",
        "We'll first need to create a Dataset from our images and labels before we can load it into a DataLoader.  If you examine the structure of your data directory that you just downloaded, you will see that the images are actually labeled by the folder in which they are located: \"No Pneumothorax\" and \"Pneumothorax\".  However, we also have the labels contained in a file \"labels.csv\" and so we are going to use this to demonstrate the creation of a custom Dataset in PyTorch.  Additionally, since our data files are DICOMs not image files, we will not be able to use the out-of-the-box PyTorch `Imagefolder()` to put them in a Dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DicomDataset(Dataset):\n",
        "    '''\n",
        "    Custom PyTorch Dataset for image classification\n",
        "    Must contain 3 parts: __init__, __len__ and __getitem__\n",
        "    '''\n",
        "\n",
        "    def __init__(self, csv_path: str, data_dir: str, class_mapper: dict, transform=None):\n",
        "        '''\n",
        "        Args:\n",
        "            csv_path (string): Path to the csv containing the image names and corresponding labels\n",
        "            data_dir (string): Path to directory containing the images\n",
        "            class_mapper (dict): Dictionary mapping string labels to numeric labels\n",
        "            transform (callable,optional): Optional transform to be applied to images\n",
        "        '''\n",
        "        self.labels_df = pd.read_csv(csv_path)\n",
        "        self.transform = transform\n",
        "        self.data_dir = data_dir\n",
        "        self.classes = self.labels_df.iloc[:,1].unique()\n",
        "        self.classmapper = class_mapper\n",
        "\n",
        "    def __len__(self):\n",
        "        '''Returns the number of images in the Dataset'''\n",
        "        return len(self.labels_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        '''\n",
        "        Returns the image and corresponding label for an input index\n",
        "        Used by PyTorch to create the iterable DataLoader\n",
        "\n",
        "        Args:\n",
        "            idx (integer): index value for which to get image and label\n",
        "        '''\n",
        "\n",
        "        # Load the image\n",
        "        img_path = os.path.join(self.data_dir,\n",
        "                                self.labels_df.iloc[idx, 0])\n",
        "        \n",
        "        # For a normal image file (jpg,png) use the below\n",
        "        # image = cv2.imread(img_path) # Use this for normal color images\n",
        "        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Use this for color images to rearrange channels BGR -> RGB\n",
        "\n",
        "        # Since we are dealing with DICOM files we instead load images using pydicom:\n",
        "        image = pydicom.dcmread(img_path) # load the dicom file\n",
        "        image = image.pixel_array # convert dicom pixel data to numpy array\n",
        "        image = Image.fromarray(image) # convert numpy array to PIL image\n",
        "\n",
        "        # Load the label\n",
        "        label = self.labels_df.iloc[idx, 1]\n",
        "        label = self.classmapper[label]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "            \n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also need to set up our transforms for the dataset, which will be applied to the images when they are loaded into our XrayDataset we created above.\n",
        "\n",
        "The transforms we apply below do the following:  \n",
        "- `Resize(224)`: resize to 224 to be consistent with model pre-trained on ImageNet (we do not want to do random cropping in this case since we want to preserve the lungs in the images)  \n",
        "- `ToTensor()`: convert pixel values to a Tensor and scale to range [0,1]  \n",
        "- `Normalize()`: standardize pixel values to have approx. mean=0 and std=1 (not required but helps model train better).  Note: we generally want to standardize using the mean and std of the distribution of the images in our training set.  However, since in this case we do not know what they are and it requires a bit of work to calculate them, we will just simplify and use 0.5,0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_transform = transforms.Compose([transforms.Resize(224),\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize(mean=[0.5],std=[0.5])])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we are ready to create our Dataset using our custom class, and then use it to load the data into a DataLoader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = 'data/siim_small'\n",
        "\n",
        "\n",
        "# Create Dataset\n",
        "train_dataset = DicomDataset(csv_path=os.path.join(data_dir,'labels.csv'),\n",
        "                            data_dir=data_dir,\n",
        "                            class_mapper=class_to_idx,\n",
        "                            transform = data_transform)\n",
        "\n",
        "# Create DataLoader\n",
        "batch_size = 8\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
        "                                             shuffle=True, num_workers=0)\n",
        "\n",
        "# Store size of training set\n",
        "dataset_size = len(train_dataset)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get next batch of training images\n",
        "images, labels = next(iter(train_loader))\n",
        "print(images.shape)\n",
        "images = images.numpy() # Convert images to numpy for display\n",
        "labels = labels.numpy()\n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "for idx in range(batch_size):\n",
        "    ax = fig.add_subplot(2, batch_size//2, idx+1, xticks=[], yticks=[])\n",
        "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
        "    ax.set_title(idx_to_class[labels[idx]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we see above, the inputs in a PyTorch DataLoader are of shape [N,C,H,W) where:  \n",
        "- N = batch size  \n",
        "- C = number of channels (1 for grayscale, 3 for RGB color)  \n",
        "- H = image height  \n",
        "- W = image width"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define our model architecture\n",
        "We will used a pre-trained ResNet18 model in this example.  However, we will need to make a couple changes to it:  \n",
        "- Since we have 2 output classes instead of 1000 (the ImageNet default), we need to replace the final fully connected layer in the network with a new layer that has 2 output units (not 1000)  \n",
        "- Since our images are black/white, we have a single input channel rather than 3.  Therefore we will replace the first layer in the network with a new convolutional layer that has `in_channels = 1`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a resnet18 pre-trained model\n",
        "model_resnet = torchvision.models.resnet18(pretrained=True)\n",
        "# Shut off autograd for all layers to freeze model so the layer weights are not trained\n",
        "for param in model_resnet.parameters():\n",
        "    param.requires_grad = False\n",
        "    \n",
        "# Replace the resnet input layer to take in grayscale images (1 input channel), since it was originally trained on color (3 input channels)\n",
        "in_channels = 1\n",
        "model_resnet.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "# Replace the resnet final layer with a new fully connected Linear layer we will train on our task\n",
        "# Number of out units is number of classes (2)\n",
        "num_ftrs = model_resnet.fc.in_features\n",
        "model_resnet.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "# Display a summary of the layers of the model and output shape after each layer\n",
        "summary(model_resnet,(images.shape[1:]),batch_size=batch_size,device=\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model,criterion,optimizer,loader,n_epochs,device):\n",
        "    \n",
        "    loss_over_time = [] # to track the loss as the network trains\n",
        "    \n",
        "    model = model.to(device) # Send model to GPU if available\n",
        "    model.train() # Set the model to training mode\n",
        "    \n",
        "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
        "        \n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        \n",
        "        for i, data in enumerate(loader):\n",
        "            # Get the input images and labels, and send to GPU if available\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # Zero the weight gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass to get outputs\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            # Calculate the loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backpropagation to get the gradients with respect to each weight\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the weights\n",
        "            optimizer.step()\n",
        "\n",
        "            # Convert loss into a scalar and add it to running_loss\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Convert loss into a scalar and add it to running_loss\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            # Track number of correct predictions\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "            \n",
        "        # Calculate and display average loss and accuracy for the epoch\n",
        "        epoch_loss = running_loss / len(train_dataset)\n",
        "        epoch_acc = running_corrects.double() / len(train_dataset)\n",
        "        print('Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
        "\n",
        "        loss_over_time.append(epoch_loss)\n",
        "\n",
        "    return loss_over_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Train the model\n",
        "n_epochs = 20\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_resnet.parameters(), lr=0.001)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cost_path = train_model(model_resnet,criterion,optimizer,train_loader,n_epochs,device)\n",
        "\n",
        "# Visualize the loss as the network trained\n",
        "plt.plot(cost_path)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display a batch of predictions\n",
        "def visualize_results(model,dataloader,device):\n",
        "    model = model.to(device) # Send model to GPU if available\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        # Get a batch of validation images\n",
        "        images, labels = next(iter(train_loader))\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        # Get predictions\n",
        "        _,preds = torch.max(model(images), 1)\n",
        "        preds = np.squeeze(preds.cpu().numpy())\n",
        "        labels = labels.cpu().numpy()\n",
        "\n",
        "    # Plot the images in the batch, along with predicted and true labels\n",
        "    fig = plt.figure(figsize=(15, 10))\n",
        "    for idx in np.arange(len(preds)):\n",
        "        ax = fig.add_subplot(2, len(preds)//2, idx+1, xticks=[], yticks=[])\n",
        "        image = images[idx]\n",
        "        image = image.permute(1,2,0).cpu().numpy() # Permute axes because im.show() expects dims [W,H,C] and PyTorch/NumPy use [C,W,H]\n",
        "        mean = np.array([0.5])\n",
        "        std = np.array([0.5])\n",
        "        image = std * image + mean\n",
        "        image = np.clip(image, 0, 1)\n",
        "        ax.imshow(image, cmap='gray')\n",
        "        ax.set_title(\"{} ({})\".format(idx_to_class[preds[idx]], idx_to_class[labels[idx]]),\n",
        "                    color=(\"green\" if preds[idx]==labels[idx] else \"red\"))\n",
        "    return\n",
        "\n",
        "visualize_results(model_resnet,train_loader,device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "cnn_basics.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
